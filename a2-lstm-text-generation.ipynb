{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import requests\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMLanguageModel Class\n",
    "This is where I define the main model I am going to use to train and generate text. I need to use an LSTM (Long Short-Term Memory) model because it's great for handling sequences like text. LSTM can remember the context of previous words and use it to predict the next word in a sequence, which is what I need for text generation. The class has an embedding layer to convert words into numbers, an LSTM layer to process the sequences, and a final layer to predict the next word in the sequence.\n",
    "\n",
    "### Forward Pass in LSTM\n",
    "The forward method describes what happens when data goes through the model. First, I convert the words (tokens) into embeddings. Then, I pass them through the LSTM to get the model’s predictions. Finally, I use a fully connected layer to output the predicted word. In this method I check how the model makes predictions.\n",
    "\n",
    "### Initializing the Hidden States\n",
    "LSTMs need to remember things from the past, so I need to initialize the hidden state and the cell state (kind of like memory) at the start. These hidden states will get updated as the model processes the data. This function just sets the initial hidden states to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)  # embedding layer\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout=dropout_rate, batch_first=True)  # lstm layer\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)  # output layer\n",
    "        \n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.hidden_dim = hid_dim  # hidden layer size\n",
    "        self.num_layers = num_layers  # num of layers\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embed = self.embedding(x)  # token to embedding\n",
    "        out, hidden = self.lstm(embed, hidden)  # lstm forward pass\n",
    "        out = self.fc(out)  # final output layer\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),  # hidden state init\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)   # cell state init\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Harry Potter Data\n",
    "I am loading the Harry Potter dataset. I using load_dataset from the Hugging Face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_harry_potter_data():\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"elricwan/HarryPotter\")  # load dataset\n",
    "    all_content = [item[\"content\"] for item in dataset[\"train\"]]  # get content from dataset\n",
    "    return train_test_split(all_content, test_size=0.2, random_state=42)  # split data into train/test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pride and Prejudice Data\n",
    "This is a similar function to load the \"Pride and Prejudice\" dataset, but this time I grab the text from a URL. I split it into lines and use the train_test_split function to split it into training and validation datasets. I need both datasets to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pride_and_prejudice_data():\n",
    "    url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # url for the text\n",
    "    response = requests.get(url)  # get data\n",
    "    text = response.text  # read content\n",
    "    return train_test_split(text.split(\"\\n\"), test_size=0.2, random_state=42)  # split into train/test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Building Vocabulary\n",
    "I use the get_tokenizer from TorchText to split text into words (tokens). After that, I build a vocabulary from the tokens, so the model knows what words are in the dataset. I also define special tokens like <unk> for unknown words and <eos> for the end of a sentence. In this step I need a way to convert words into numbers that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_build_vocab(train_data):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")  # use basic english tokenizer\n",
    "    tokenized_data = [tokenizer(text) for text in train_data]  # tokenize data\n",
    "    vocab = build_vocab_from_iterator(tokenized_data, min_freq=5, specials=[\"<unk>\", \"<eos>\"])  # build vocab todo: change min freq\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])  # set default to <unk>\n",
    "    return vocab, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data to Batches\n",
    "\n",
    "Now that I have tokenized the text and built the vocabulary, I need to turn the data into batches for training. This function takes the text, tokenizes it, and converts it into a tensor that the model can process. I also make sure that the batch size is consistent, and the data is in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_batches(data, vocab, tokenizer, batch_size):\n",
    "    tokenized_data = [torch.tensor([vocab[token] for token in tokenizer(text)] + [vocab[\"<eos>\"]]) for text in data]  # tokenize and convert to indices\n",
    "    data_tensor = torch.cat(tokenized_data)  # concatenate all tokens\n",
    "    num_batches = data_tensor.size(0) // batch_size  # calculate batches\n",
    "    data_tensor = data_tensor[:num_batches * batch_size]  # ensure even batch size\n",
    "    data_tensor = data_tensor.view(batch_size, -1)  # reshape into batch format\n",
    "    return data_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "This function is responsible for training the model. It loops through the data, computes the loss, and adjusts the weights of the model to minimize the loss. I use gradient clipping to avoid exploding gradients, which can happen during training when the gradients become too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    model.train()  # set model to train mode\n",
    "    epoch_loss = 0  # track loss\n",
    "    hidden = model.init_hidden(batch_size, device)  # init hidden state\n",
    "    num_batches = data.size(1) // seq_len  # calculate batches\n",
    "    for idx in range(0, data.size(1) - seq_len, seq_len):  # loop through data\n",
    "        src = data[:, idx:idx + seq_len].to(device)  # get input sequence\n",
    "        target = data[:, idx + 1:idx + seq_len + 1].to(device)  # target sequence\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        output, hidden = model(src, hidden)  # model prediction\n",
    "        hidden = tuple(h.detach() for h in hidden)  # detach hidden state\n",
    "        output = output.reshape(-1, output.size(-1))  # reshape output for loss\n",
    "        target = target.reshape(-1)  # reshape target for loss\n",
    "        loss = criterion(output, target)  # calculate loss\n",
    "        loss.backward()  # backpropagate\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # gradient clipping\n",
    "        optimizer.step()  # update weights\n",
    "        epoch_loss += loss.item()  # accumulate loss\n",
    "    return epoch_loss / num_batches  # return avg loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "After training the model, I need to check how well it's doing. This function evaluates the model using the validation data and calculates the loss. I don’t update the model weights during evaluation, so I set the model to evaluation mode and don’t track gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    model.eval()  # set model to eval mode\n",
    "    epoch_loss = 0  # track loss\n",
    "    hidden = model.init_hidden(batch_size, device)  # init hidden state\n",
    "    num_batches = data.size(1) // seq_len  # calculate batches\n",
    "    with torch.no_grad():  # no gradients during eval\n",
    "        for idx in range(0, data.size(1) - seq_len, seq_len):  # loop through data\n",
    "            src = data[:, idx:idx + seq_len].to(device)  # get input sequence\n",
    "            target = data[:, idx + 1:idx + seq_len + 1].to(device)  # target sequence\n",
    "            output, hidden = model(src, hidden)  # model prediction\n",
    "            hidden = tuple(h.detach() for h in hidden)  # detach hidden state\n",
    "            output = output.reshape(-1, output.size(-1))  # reshape output\n",
    "            target = target.reshape(-1)  # reshape target\n",
    "            loss = criterion(output, target)  # calculate loss\n",
    "            epoch_loss += loss.item()  # accumulate loss\n",
    "    return epoch_loss / num_batches  # return avg loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing and saving model\n",
    "\n",
    "This function handles the entire training process for a given model. First, it loads the dataset and prepares the training and validation data. Then, it initializes the model, optimizer, and loss function. The function trains the model for a set number of epochs, calculating the training and validation losses at each step to monitor the performance. After each epoch, it saves the model if the validation loss improves. This function combines all the steps needed to train and save the model, making the training process automated and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(data_loader, model_filename, vocab_filename, tokenizer_filename):\n",
    "    train_data, valid_data = data_loader()  # load data\n",
    "    print(f\"Training data size: {len(train_data)}\")  # print train data size\n",
    "    print(f\"Validation data size: {len(valid_data)}\")  # print validation data size\n",
    "\n",
    "    vocab, tokenizer = tokenize_and_build_vocab(train_data)  # tokenizing and vocab building\n",
    "\n",
    "    batch_size = 32  # batch size\n",
    "    train_batches = data_to_batches(train_data, vocab, tokenizer, batch_size)  # convert data to batches\n",
    "    valid_batches = data_to_batches(valid_data, vocab, tokenizer, batch_size)  # convert validation data to batches\n",
    "\n",
    "    # torch.set_num_threads(os.cpu_count())  # Set to all available cores\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    emb_dim = 1024                # 400 in the paper\n",
    "    hid_dim = 1024                # 1150 in the paper\n",
    "    num_layers = 2                # 3 in the paper\n",
    "    dropout_rate = 0.65              \n",
    "    lr = 1e-3   \n",
    "    clip = 1  # gradient clipping\n",
    "    n_epochs = 5  # number of epochs\n",
    "    seq_len = 32  # sequence length\n",
    "\n",
    "    # vocab_size = len(vocab)\n",
    "    # emb_dim = 256\n",
    "    # hid_dim = 256\n",
    "    # num_layers = 1\n",
    "    # dropout_rate = 0.3\n",
    "    # lr = 1e-3\n",
    "    # n_epochs = 1\n",
    "    # seq_len = 25\n",
    "    # clip = 0.25\n",
    "    \n",
    "    print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "    model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)  # initialize model\n",
    "    optimizer  = optim.Adam(model.parameters(), lr=lr) # Adam optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # loss function\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over epochs\n",
    "        train_loss = train(model, train_batches, optimizer, criterion, batch_size, seq_len, clip, device)  # training\n",
    "        valid_loss = evaluate(model, valid_batches, criterion, batch_size, seq_len, device)  # evaluation\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")  # epoch info\n",
    "        print(f\"\\tTrain Perplexity: {torch.exp(torch.tensor(train_loss)):.3f}\")  # print train perplexity\n",
    "        print(f\"\\tValid Perplexity: {torch.exp(torch.tensor(valid_loss)):.3f}\")  # print validation perplexity\n",
    "\n",
    "\n",
    "     # Save model and vocab using torch.save\n",
    "    torch.save({\"model_state\": model.state_dict(), \"vocab\": vocab}, model_filename)\n",
    "    \n",
    "    # Save tokenizer and vocab using pickle\n",
    "    with open(vocab_filename, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    with open(tokenizer_filename, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Model, vocab, and tokenizer saved to {model_filename}, {vocab_filename}, and {tokenizer_filename}\")\n",
    "    return model, valid_batches, criterion, batch_size, seq_len, vocab, tokenizer  # return model and data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "This function generates text given a prompt. I pass the prompt through the model and sample the next words one by one. The temperature parameter controls how random the predictions are. A higher temperature leads to more randomness, and a lower temperature makes the model more confident in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)  # set random seed if given\n",
    "    \n",
    "    model.eval()  # set model to eval mode\n",
    "    tokens = tokenizer(prompt)  # tokenize the prompt\n",
    "    indices = [vocab[t] for t in tokens]  # convert tokens to indices\n",
    "    batch_size = 1  # batch size is 1 for generation\n",
    "    hidden = model.init_hidden(batch_size, device)  # initialize hidden state\n",
    "    \n",
    "    with torch.no_grad():  # no gradients during generation\n",
    "        for i in range(max_seq_len):  # generate for max_seq_len\n",
    "            src = torch.LongTensor([indices]).to(device)  # input sequence\n",
    "            prediction, hidden = model(src, hidden)  # model prediction\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  # apply temperature\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()  # sample from probabilities\n",
    "\n",
    "            while prediction == vocab['<unk>']:  # avoid unknown tokens\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:  # stop if end of sentence token\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)  # append predicted word index\n",
    "\n",
    "    itos = vocab.get_itos()  # get index-to-token mapping\n",
    "    tokens = [itos[i] for i in indices]  # convert indices back to tokens\n",
    "    return tokens  # return generated tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training\n",
    "This chunk trains two separate models: one for the Harry Potter dataset and one for Pride and Prejudice. First, I load and train the Harry Potter model using train_and_save_model(), saving it to harry_potter_lstm.pt. Then, I repeat the same process for the Pride and Prejudice model, saving it to pride_prejudice_lstm.pt. The function also returns the necessary data, loss function, batch size, sequence length, vocabulary, and tokenizer for both models. This train and save both models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Harry Potter Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 6\n",
      "Validation data size: 2\n",
      "Vocabulary Size: 13034\n",
      "Epoch 1/5\n",
      "\tTrain Perplexity: 115.891\n",
      "\tValid Perplexity: 74.714\n",
      "Epoch 2/5\n",
      "\tTrain Perplexity: 59.298\n",
      "\tValid Perplexity: 58.712\n",
      "Epoch 3/5\n",
      "\tTrain Perplexity: 43.454\n",
      "\tValid Perplexity: 52.555\n",
      "Epoch 4/5\n",
      "\tTrain Perplexity: 34.643\n",
      "\tValid Perplexity: 49.894\n",
      "Epoch 5/5\n",
      "\tTrain Perplexity: 29.026\n",
      "\tValid Perplexity: 48.190\n",
      "Model, vocab, and tokenizer saved to harry_potter_lstm.pt, harry_potter_vocab.pkl, and harry_potter_tokenizer.pkl\n",
      "Training Pride and Prejudice Model\n",
      "Training data size: 11627\n",
      "Validation data size: 2907\n",
      "Vocabulary Size: 1912\n",
      "Epoch 1/5\n",
      "\tTrain Perplexity: 158.256\n",
      "\tValid Perplexity: 96.283\n",
      "Epoch 2/5\n",
      "\tTrain Perplexity: 81.226\n",
      "\tValid Perplexity: 72.676\n",
      "Epoch 3/5\n",
      "\tTrain Perplexity: 63.721\n",
      "\tValid Perplexity: 67.398\n",
      "Epoch 4/5\n",
      "\tTrain Perplexity: 54.038\n",
      "\tValid Perplexity: 64.144\n",
      "Epoch 5/5\n",
      "\tTrain Perplexity: 46.867\n",
      "\tValid Perplexity: 63.702\n",
      "Model, vocab, and tokenizer saved to pride_prejudice_lstm.pt, pride_prejudice_vocab.pkl, and pride_prejudice_tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Harry Potter Model\")\n",
    "hp_model, hp_valid_data, hp_criterion, hp_batch_size, hp_seq_len, hp_vocab, hp_tokenizer = train_and_save_model(\n",
    "    load_harry_potter_data, \"harry_potter_lstm.pt\", \"harry_potter_vocab.pkl\", \"harry_potter_tokenizer.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Training Pride and Prejudice Model\")\n",
    "pp_model, pp_valid_data, pp_criterion, pp_batch_size, pp_seq_len, pp_vocab, pp_tokenizer = train_and_save_model(\n",
    "    load_pride_and_prejudice_data, \"pride_prejudice_lstm.pt\", \"pride_prejudice_vocab.pkl\", \"pride_prejudice_tokenizer.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Testing the text generation capability of both the Harry Potter and Pride and Prejudice models. For each model, I use a prompt and generate text with varying temperatures (from 0.5 to 1.0) to explore how temperature affects the creativity and randomness of the output. I evaluate how well each model can generate text that continues from the given prompt, with different temperatures providing insights into the model's ability to produce diverse or more predictable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter Model Text Generation:\n",
      "Temperature 0.5\n",
      "harry potter is merciful , and i hope you’re biding a hundred thousand things , but i have already presented my forces to divine support . and so i shall be sure that\n",
      "\n",
      "Temperature 0.7\n",
      "harry potter is really good ! ” “i didn’t mean to change , ” said harry , “but i don’t understand . . . . ” and so soon it was , though\n",
      "\n",
      "Temperature 0.75\n",
      "harry potter is really good ! ” “i beg your pardon , potter , ” said dumbledore , but he did not care what sirius was saying . harry glanced in the mirror\n",
      "\n",
      "Temperature 0.8\n",
      "harry potter is really good ! ” “i beg your pardon , potter , ” said dumbledore , but dumbledore smiled . “the accused thing he and me . . . ” “it\n",
      "\n",
      "Temperature 1.0\n",
      "harry potter is really good ! ” “but you’re not supposed to be hiding out for that , it seems four years later —” “but i am afraid i did in , though\n",
      "\n",
      "Pride and Prejudice Model Text Generation:\n",
      "Temperature 0.5\n",
      "charlotte lucas and her father were at\n",
      "\n",
      "Temperature 0.7\n",
      "charlotte lucas and her father were at\n",
      "\n",
      "Temperature 0.75\n",
      "charlotte lucas and her father were at\n",
      "\n",
      "Temperature 0.8\n",
      "charlotte lucas and her father were at\n",
      "\n",
      "Temperature 1.0\n",
      "charlotte lucas and her father were sensible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the text generation with a prompt for Harry Potter model\n",
    "prompt_hp = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "print(\"Harry Potter Model Text Generation:\")\n",
    "for temperature in temperatures:\n",
    "    generation_hp = generate(prompt_hp, max_seq_len, temperature, hp_model, hp_tokenizer, hp_vocab, \"mps\", seed)\n",
    "    print(f\"Temperature {temperature}\\n{' '.join(generation_hp)}\\n\")\n",
    "\n",
    "# Test the text generation with a prompt for Pride and Prejudice model\n",
    "prompt_pp = 'Charlotte Lucas and her father '\n",
    "print(\"Pride and Prejudice Model Text Generation:\")\n",
    "for temperature in temperatures:\n",
    "    generation_pp = generate(prompt_pp, max_seq_len, temperature, pp_model, pp_tokenizer, pp_vocab, \"mps\", seed)\n",
    "    print(f\"Temperature {temperature}\\n{' '.join(generation_pp)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
